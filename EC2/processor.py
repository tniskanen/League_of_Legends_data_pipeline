import time
import sys
import psutil 

try:
    print("Testing imports...")
    from Utils.api import match, handle_api_response
    from Utils.S3 import send_match_json, pull_s3_object, upload_to_s3, alter_s3_file
    from Utils.logger import get_logger
    logger = get_logger(__name__)
    print("‚úÖ All imports successful")
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print("Available modules in current directory:")
    print(f"üõë Matchlist processing failed due to import error")
    print(f"üìã Manual intervention required: Check container dependencies")
    sys.exit(7)
except Exception as e:
    print(f"‚ùå Unexpected import error: {e}")
    print(f"Exception type: {type(e).__name__}")
    print(f"Exception details: {str(e)}")
    import traceback
    print(f"Full traceback: {traceback.format_exc()}")
    print(f"üõë Matchlist processing failed due to unexpected import error")
    print(f"üìã Manual intervention required: Check container setup")
    sys.exit(7)

def run_processor(config, matchlist):

    print(f"üöÄ Running {__name__}.py")
    print(f"Python version: {sys.version}")

    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

    # Retry logic for S3 pull - most failures are temporary
    MAX_RETRIES = 3
    matchlist_data = None
    
    for attempt in range(MAX_RETRIES):
        print(f"üì• Attempting to pull matchlist from S3 (attempt {attempt + 1}/{MAX_RETRIES})...")
        matchlist_data = pull_s3_object(config['BUCKET'], matchlist)
        
        if matchlist_data is not None:
            print(f"‚úÖ Successfully pulled matchlist on attempt {attempt + 1}")
            break
        
        if attempt < MAX_RETRIES - 1:
            print(f"‚ö†Ô∏è Pull failed, waiting 10 seconds before retry...")
            time.sleep(10)
    
    if matchlist_data is None:
        print(f"‚ùå Failed to pull matchlist from S3 after {MAX_RETRIES} attempts: {matchlist}")
        print(f"‚ö†Ô∏è This could be a temporary S3 issue or corrupted file")
        print(f"üîÑ Preserving matchlist and exiting for manual intervention")
        print(f"üìã Manual action required:")
        print(f"   1. Check if {matchlist} exists in S3")
        print(f"   2. If corrupted, regenerate matchlist for epoch {config['start_epoch']} to {config['end_epoch']}")
        print(f"   3. If temporary issue, retry this container")
        print(f"üõë Exiting with error code 7 - manual intervention needed")
        exit(7)
    
    uniqueMatches = matchlist_data['matchlist']
    player_rank_map = matchlist_data['ranked_map']

    #uploading matchlist to s3
    try:
        pm_key = f'player-maps/player-map_{config["start_epoch"]}_{config["end_epoch"]}_.json'
        upload_to_s3(config['BUCKET'], pm_key, player_rank_map)
        print(f"‚úÖ player-map uploaded to: {pm_key}")
    except Exception as e:
        print(f"‚ùå CRITICAL: Failed to upload player-map: {e}")
        print(f"üõë Cannot proceed without player-map - exiting with error")
        sys.exit(1)  # Critical failure - exit code 1

    print(f"Starting data processing at {time.strftime('%Y-%m-%d %H:%M:%S')}")

    successful_matches = 0
    total = 0
    no_data = 0
    matches = []
    active_threads = []
    current_index = 0  # Track current position for leftover handling

    try:
        for i, match_id in enumerate(uniqueMatches):
            current_index = i  # Update current position
            # Check API key expiration before processing each match
            current_time = int(time.time())
            if current_time >= int(config['API_KEY_EXPIRATION']):
                print(f"‚ö†Ô∏è API key expired at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(int(config['API_KEY_EXPIRATION'])))}!")
                print(f"Current time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_time))}")
                
                # Get unprocessed matches (from current index onwards)
                unprocessed_matches = list(uniqueMatches)[i + 1:]
                print(f"Saving {len(unprocessed_matches)} unprocessed matches to S3...")
                
                # Create data to upload with unprocessed matches and player rank map
                data_to_upload = {
                    "ranked_map": player_rank_map,
                    "matchlist": unprocessed_matches
                }
                
                # Upload leftovers to S3
                key = f'backfill/leftovers/leftovers_{config["start_epoch"]}_{config["end_epoch"]}_{len(unprocessed_matches)}_matches.json'
                upload_to_s3(config['BUCKET'], key, data_to_upload)
                print(f"‚úÖ Unprocessed matches saved to: {key}")

                print(f"üõë Processing stopped due to API key expiration. Processed {i}/{len(uniqueMatches)} matches.")
                break
            
            # Progress indicator every 1000 matches
            if i % 1000 == 0:
                print(f"  Progress: {i}/{len(uniqueMatches)} matches processed")
                
            temp_data = match(match_id, config['API_KEY'])
            if handle_api_response(temp_data, func_name='match') is None:
                no_data += 1
                continue
                
            for participant in temp_data['info']['participants']:
                puuid = participant.get('puuid')

                rank_info = player_rank_map.get(puuid)
                if rank_info:
                    participant['tier'] = rank_info['tier']
                    participant['player_rank'] = rank_info['player_rank']
                    participant['leaguePoints'] = rank_info['leaguePoints']
                else:
                    participant['tier'] = 'UNKNOWN'
                    participant['player_rank'] = None
                    participant['leaguePoints'] = None

            temp_data['source'] = config['source']
            matches.append(temp_data)
            successful_matches += 1
            total += 1

            # Upload every 500 successful matches
            if successful_matches % 500 == 0:
                print(f"Uploading batch of {successful_matches} matches to S3 (total processed: {total})")
                thread = send_match_json(data=matches.copy(), bucket=config['BUCKET'], source=config['source'])  # Explicit copy
                if thread:
                    active_threads.append(thread)
                matches = []

    except Exception as e:
        print(f"‚ùå ERROR during match processing: {e}")
        print(f"‚ùå Exception type: {type(e).__name__}")
        print(f"‚ùå Exception details: {str(e)}")
        import traceback
        print(f"‚ùå Full traceback: {traceback.format_exc()}")
        
        # Always handle unprocessed matches due to unexpected error
        unprocessed_matches = list(uniqueMatches)[current_index + 1:] if current_index < len(uniqueMatches) - 1 else []
        print(f"‚ö†Ô∏è Error occurred during processing. Saving {len(unprocessed_matches)} unprocessed matches to S3...")
        
        # Create data to upload with unprocessed matches and player rank map
        data_to_upload = {
            "ranked_map": player_rank_map,
            "matchlist": unprocessed_matches
        }
        
        # Upload leftovers to S3
        key = f'backfill/leftovers/leftovers_{config["start_epoch"]}_{config["end_epoch"]}_{len(unprocessed_matches)}_matches.json'
        upload_to_s3(config['BUCKET'], key, data_to_upload)
        print(f"‚úÖ Unprocessed matches saved to: {key}")

    # Upload remaining matches
    if matches:
        print(f"Uploading final batch of {len(matches)} matches")
        thread = send_match_json(data=matches, bucket=config['BUCKET'], source=config['source'])
        if thread:
            active_threads.append(thread)

    # Wait for all uploads
    print(f"Waiting for {len(active_threads)} upload threads to complete...")
    for i, thread in enumerate(active_threads):
        print(f"  Waiting for upload thread {i+1}/{len(active_threads)}")
        thread.join()

    print("All uploads completed!")
    print(f"Matches with no data: {no_data}")

    # Always delete matchlist - it's either fully processed or stored in leftovers
    alter_s3_file(config['BUCKET'], matchlist, 'delete')
    print(f"‚úÖ Matchlist deleted from S3")
    
    end_time = time.time()
    end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

    print(f"üéâ JOB COMPLETED!")
    print(f"Runtime: {end_time - start_time:.2f} seconds")
    print(f"Memory usage: {start_memory:.1f}MB -> {end_memory:.1f}MB")
    print(f"Total matches processed: {total}")
    print(f"Upload batches: {len(active_threads)}")
    return